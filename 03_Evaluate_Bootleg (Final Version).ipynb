{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os.path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_dir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure error rate (single pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSheetRefLocs(scoreid, striplens, changeDPI = False):\n",
    "    \n",
    "    # get annotation file\n",
    "    annot_dir = 'annot_data'\n",
    "    piece = scoreid.split('_')\n",
    "    annot_file_beats = '%s/%s_%s_beats.csv' % (annot_dir, piece[0], piece[1])\n",
    "    df_all = pd.read_csv(annot_file_beats)\n",
    "    \n",
    "    # calculate global pixel position\n",
    "    scoreid = piece[1]+'_'+piece[2]\n",
    "    df = df_all.loc[df_all.score == scoreid]\n",
    "    pixelOffset = np.cumsum([0] + striplens)  # cumulative pixel offset for each strip\n",
    "    stripsPerPage = [df.loc[df.page == i,'strip'].max() for i in range(df.page.max()+1) ]\n",
    "    stripOffset = np.cumsum([0] + stripsPerPage)\n",
    "    stripIdx = stripOffset[df.page] + df.strip - 1  # cumulative strip index\n",
    "    if changeDPI:\n",
    "        hpixlocs = pixelOffset[stripIdx] + (df.hpixel  * 100 // 72)\n",
    "    else:\n",
    "        hpixlocs = pixelOffset[stripIdx] + df.hpixel\n",
    "\n",
    "    return hpixlocs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMidiRefLocs(midiID, scoreID, fs, zeroPad, timeMapping):\n",
    "    \n",
    "    annot_file = 'annot_data/midi/' + midiID + '.csv'\n",
    "    timeStamps = []\n",
    "    with open(annot_file, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            if row[0] != '-':\n",
    "                timeStamps.append(float(row[0]))\n",
    "            else:\n",
    "                timeStamps.append(float('inf'))\n",
    "    timeStamps = np.array(timeStamps)\n",
    "    \n",
    "    # convert to mapped frames\n",
    "    frames = (timeStamps - timeStamps[0]) * fs\n",
    "    mapped_frms = [i for i, elem in enumerate(timeMapping) if elem != -1]\n",
    "    orig_frms = [elem for elem in timeMapping if elem != -1]\n",
    "    orig_frms = np.array(orig_frms) - orig_frms[0]\n",
    "    mapped_interp = np.interp(frames, orig_frms, mapped_frms)\n",
    "    padded = mapped_interp + zeroPad\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcPredErrors(hyp_file, predType, changeDPI = False):\n",
    "    \n",
    "    # load hypothesis data\n",
    "    print(hyp_file)\n",
    "    dhyp = pickle.load(open(hyp_file, 'rb'))\n",
    "    fs = dhyp['fs']\n",
    "    zeroPad = dhyp['zeroPad']\n",
    "    timeMapping = dhyp['timeMapping']\n",
    "    striplens = dhyp['striplens']\n",
    "    wp = dhyp['wp']\n",
    "    \n",
    "    striplens = [s/3 for s in striplens]\n",
    "    \n",
    "    # get ground truth annotations\n",
    "    scoreID = os.path.splitext(os.path.basename(hyp_file))[0] # e.g. chopin_op68no3_v1.pkl\n",
    "    midiID = scoreID.split('_')[0] + '_' + scoreID.split('_')[1] # chopin_op68no3\n",
    "    sheet_ref_beats = getSheetRefLocs(scoreID, striplens, changeDPI)\n",
    "    midi_ref_beats = getMidiRefLocs(midiID, scoreID, fs, zeroPad, timeMapping)\n",
    "\n",
    "    # get beat predictions\n",
    "    if predType == 'wp':\n",
    "        sheet_preds = wp[:,0]\n",
    "        midi_preds = wp[:,1]\n",
    "    elif predType == 'globallinear':\n",
    "        sheet_preds = wp[[0,-1],0]\n",
    "        midi_preds = wp[[0,-1],1]\n",
    "    else:\n",
    "        print('Unrecognized prediction type: %s' % predType)\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # calculate prediction errors\n",
    "    hypFrames = np.interp(sheet_ref_beats, sheet_preds, midi_preds)\n",
    "    hypPixels = np.interp(midi_ref_beats, midi_preds, sheet_preds)\n",
    "    minLen = np.min((len(hypFrames), len(hypPixels)))\n",
    "    errsFrames = hypFrames[0:minLen] - midi_ref_beats[0:minLen]\n",
    "    errsTime = errsFrames * 1000.0 / fs  # in ms\n",
    "    errsPixels = hypPixels[0:minLen] - sheet_ref_beats[0:minLen]    \n",
    "    \n",
    "    if verbose:\n",
    "        for i in range(minLen):\n",
    "            print(hypFrames[i], midi_ref_beats[i])\n",
    "        print('---------------------')\n",
    "        for i in range(minLen):\n",
    "            print(hypPixels[i], sheet_ref_beats[i])\n",
    "        \n",
    "    return errsTime, errsPixels, midi_ref_beats, sheet_ref_beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcErrorStats(errs_raw, tols, isSingle = False):\n",
    "    if isSingle:\n",
    "        errs = errs_raw\n",
    "    else:\n",
    "        errs = np.array([err for sublist in errs_raw for err in sublist])\n",
    "    errs = errs[~np.isnan(errs)] # when beat is not annotated, value is nan\n",
    "    errorRates = []\n",
    "    for tol in tols:\n",
    "        toAdd = np.sum(np.abs(errs) > tol) * 1.0 / len(errs)\n",
    "        errorRates.append(toAdd)\n",
    "    return errorRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcErrorStats_globalLinear(errs_raw, tols):\n",
    "    \n",
    "    # accumulate only the non-downbeat predictions\n",
    "    errs = []\n",
    "    for sublist in errs_raw:\n",
    "        sum1 = np.nansum(np.abs(sublist[0::3]))\n",
    "        sum2 = np.nansum(np.abs(sublist[1::3]))\n",
    "        sum3 = np.nansum(np.abs(sublist[2::3]))\n",
    "        minMod = np.argmin([sum1, sum2, sum3])\n",
    "        toAdd = [e for i, e in enumerate(sublist) if i % 3 != minMod]\n",
    "        errs.extend(toAdd)\n",
    "    \n",
    "    # score as before\n",
    "    errs = np.array(errs)\n",
    "    errs = errs[~np.isnan(errs)] # when beat is not annotated, value is nan\n",
    "    errorRates = []\n",
    "    for tol in tols:\n",
    "        toAdd = np.sum(np.abs(errs) > tol) * 1.0 / len(errs)\n",
    "        errorRates.append(toAdd)\n",
    "    return errorRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotErrorRateSingle(hyp_file, title = None):\n",
    "\n",
    "    # calculate error rates\n",
    "    errsTime, errsPixels, _, _ = calcPredErrors(hyp_file, 'wp',True)\n",
    "    tols_ms = range(3000)\n",
    "    tols_px = range(833) #original: 200\n",
    "    pixelErrorRates = calcErrorStats(errsPixels, tols_px, isSingle = True)\n",
    "    timeErrorRates = calcErrorStats(errsTime, tols_ms, isSingle = True)\n",
    "\n",
    "    # plot error curves\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(tols_ms, 100.0*np.array(timeErrorRates))\n",
    "    plt.ylim((0,100))\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(tols_px, 100.0*np.array(pixelErrorRates))\n",
    "    plt.ylim((0,100))\n",
    "    plt.xlabel('Error Tolerance (pixels)')\n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return timeErrorRates[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err_list = []\n",
    "all_hyp = sorted(glob.glob(pkl_dir+'hyp_align/*'))\n",
    "for hyp_file in all_hyp:\n",
    "    err_list.append((plotErrorRateSingle(hyp_file, os.path.basename(hyp_file)), hyp_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure error rate (all pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcAllErrors(hyp_dir, predType, songlist):    \n",
    "    \n",
    "    hyp_files = []\n",
    "    for piece in songlist:\n",
    "        hyp_files = hyp_files + sorted(glob.glob('%s%s/%s*.pkl' % (pkl_dir,hyp_dir, piece)))\n",
    "    #hyp_files = [f for f in hyp_files if 'pid9140-05' not in f] # omit files with structural differences\n",
    "    time_errs = []\n",
    "    pixel_errs = []\n",
    "    debug = {}\n",
    "    \n",
    "    for hyp_file in hyp_files: # e.g. 'hyp_align/op68no3_v1--pid1263b-19-avgtapsd0.pkl'\n",
    "\n",
    "        # calc errors\n",
    "        errs_t, errs_l, midi_ref, sheet_ref = calcPredErrors(hyp_file, predType, True)\n",
    "        time_errs.append(errs_t)\n",
    "        pixel_errs.append(errs_l)\n",
    "        \n",
    "        # save debug info\n",
    "        basename = os.path.splitext(os.path.basename(hyp_file))[0]\n",
    "        scoreId = basename.split('--')[0]\n",
    "        debug[scoreId] = sheet_ref\n",
    "        debug[basename] = midi_ref\n",
    "    \n",
    "    with open('test_mzk.pkl', 'wb') as f:\n",
    "        pickle.dump(debug, f)\n",
    "        \n",
    "    return time_errs, pixel_errs, hyp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songlist = ['brahms','chopin', 'clementi', 'debussy', 'mendelssohn', 'mozart', 'schubert','tchaikovsky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_errs_bs, pixel_errs_bs, hyp_files = calcAllErrors('hyp_align', 'wp', songlist) # bootleg synthesis\n",
    "time_errs_b1, pixel_errs_b1, _ = calcAllErrors('hyp_align', 'globallinear', songlist) # baseline 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot error curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPrunedErrors(errs, hyp_files):\n",
    "    errs_pruned = [l for i, l in enumerate(errs) if '_v4' not in hyp_files[i]] # omits poor quality sheet images \n",
    "    return errs_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcAvgBeatLength_px():\n",
    "    #with open('gt.pkl','rb') as f:\n",
    "    with open('test_nonmzk.pkl','rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    accum = 0\n",
    "    cnt = 0\n",
    "    for key in d:\n",
    "        if '--' not in d:\n",
    "            beats = d[key]\n",
    "            diff = beats[1:] - beats[0:-1]\n",
    "            accum += np.nansum(diff)\n",
    "            cnt += len(diff) - np.sum(np.isnan(diff))\n",
    "    print('Avg beat length: %.1f pixels' % (accum*1.0/cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcAvgBeatLength_ms():\n",
    "    annotfiles = glob.glob('annot_audio/op*/*.txt')\n",
    "    accum = 0\n",
    "    cnt = 0\n",
    "    for annot_file in annotfiles:\n",
    "        df = pd.read_csv(annot_file, header=None, sep='\\t', comment='#')\n",
    "        beats = df.loc[:,0].values\n",
    "        diff = beats[1:] - beats[0:-1]\n",
    "        accum += np.sum(diff)\n",
    "        cnt += len(diff)\n",
    "    print('Avg beat length: %.3f ms' % (accum * 1000.0 / cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tols_px = range(51)\n",
    "tols_ms = range(501)\n",
    "\n",
    "# baseline 1\n",
    "timeErrorRates_b1 = calcErrorStats(time_errs_b1, tols_ms)\n",
    "pixelErrorRates_b1 = calcErrorStats(pixel_errs_b1, tols_px)\n",
    "\n",
    "# bootleg synthesis\n",
    "timeErrorRates_bs = calcErrorStats(time_errs_bs, tols_ms)\n",
    "pixelErrorRates_bs = calcErrorStats(pixel_errs_bs, tols_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results/errorData_real_bootleg.pkl','wb') as f:\n",
    "    pickle.dump([pixel_errs_bs, pixel_errs_b1, time_errs_bs, time_errs_b1],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame error curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calcAvgBeatLength_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.axvline(x=320, color = '.85')\n",
    "plt.plot(tols_ms, 100.0*np.array(timeErrorRates_b1), 'k-.', label='Global Linear')\n",
    "#plt.plot(tols_ms, 100.0*np.array(timeErrorRates_b2), 'k--', label='Downbeat Oracle + Linear')\n",
    "plt.plot(tols_ms, 100.0*np.array(timeErrorRates_bs), 'k-', label='Bootleg Synthesis')\n",
    "#plt.plot(tols_ms, 100.0*np.array(timeErrorRates_bs_pruned), '.5', label='Bootleg Synthesis (pruned)')\n",
    "plt.ylim((0,100))\n",
    "plt.xlabel('Error Tolerance (ms)')\n",
    "plt.ylabel('Error Rate (%)')\n",
    "plt.legend()\n",
    "plt.savefig('figs/error_curves_bootleg.png', dpi=300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixel error curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calcAvgBeatLength_px()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.axvline(x=30, color = '.85')\n",
    "plt.plot(tols_px, 100.0*np.array(pixelErrorRates_b1), 'k-.', label='Global Linear')\n",
    "#plt.plot(tols_px, 100.0*np.array(pixelErrorRates_b2), 'k--', label='Downbeat Oracle + Linear')\n",
    "plt.plot(tols_px, 100.0*np.array(pixelErrorRates_bs), 'k-', label='Bootleg Synthesis')\n",
    "#plt.plot(tols_px, 100.0*np.array(pixelErrorRates_bs_pruned), '.5', label='Bootleg Synthesis (pruned)')\n",
    "plt.ylim((0,100))\n",
    "plt.xlabel('Error Tolerance (pixels)')\n",
    "plt.ylabel('Error Rate (%)')\n",
    "plt.legend()\n",
    "plt.savefig('figs/error_curves_bootleg_px.png', dpi=300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
